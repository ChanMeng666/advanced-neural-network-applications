{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50524d2a9a49b225",
   "metadata": {},
   "source": [
    "# Part 1: Perceptron\n",
    "## (ii)\n",
    "### Method 1\n",
    "\n",
    "- Uses raw data without normalisation.\n",
    "- Weights and biases start from zero and rely entirely on the data and the training process to adjust naturally.\n",
    "- A higher learning rate (0.1) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebff69510b7ee1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T06:37:48.044549Z",
     "start_time": "2024-08-21T06:37:47.796148Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fractions import Fraction\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('Fish_data.csv')\n",
    "X = df.iloc[:, 0:2].values  # Input features\n",
    "y = df.iloc[:, 2].values    # Labels (0: Canadian, 1: Alaskan)\n",
    "\n",
    "# Initialize weights and bias\n",
    "w = np.zeros(2)\n",
    "b = 0  # Adjusted initial bias\n",
    "learning_rate = 0.1  # Increased learning rate for faster convergence\n",
    "epochs = 200\n",
    "\n",
    "# Perceptron learning rule\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X)):\n",
    "        # Compute the linear combination and apply the step function\n",
    "        z = np.dot(X[i], w) + b\n",
    "        pred = 1 if z >= 0 else 0\n",
    "        # Update weights and bias if there is a misclassification\n",
    "        if y[i] != pred:\n",
    "            error = y[i] - pred\n",
    "            w += learning_rate * error * X[i]\n",
    "            b += learning_rate * error\n",
    "\n",
    "# Define function to plot decision boundary and textbook equations\n",
    "def plot_decision_boundary(X, y, w, b):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='blue', marker='o', label='Canadian')\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='orange', marker='x', label='Alaskan')\n",
    "    \n",
    "    # Plot perceptron boundary\n",
    "    x_values = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 200)\n",
    "    \n",
    "    # y_values = -(b + w[0] * x_values) / w[1]\n",
    "    # plt.plot(x_values, y_values, 'g', label=f'Chan Meng Trained: y = {-w[0]/w[1]:.2f}x + {-b/w[1]:.2f}')\n",
    "    \n",
    "    # The fractions.Fraction class is used to convert the weights and bias coefficients of the model's training results so that the linear equation is presented as a minimal fraction.\n",
    "    # The use of the limit_denominator() method ensures that the fractions are concise and easy to understand, which helps improve the readability and accuracy of the equations.\n",
    "    slope = Fraction(-w[0]/w[1]).limit_denominator()\n",
    "    intercept = Fraction(-b/w[1]).limit_denominator()\n",
    "    y_values = slope * x_values + intercept\n",
    "    plt.plot(x_values, y_values, 'g', label=f'Chan Meng Trained: y = {slope}x + {intercept}')\n",
    "    \n",
    "    # Textbook equations\n",
    "    y_textbook_1 = 0.026 + 3.31 * x_values  # Equation 2.15a\n",
    "    y_textbook_2 = 106.9 + 2.5 * x_values   # Equation 2.15b\n",
    "    plt.plot(x_values, y_textbook_1, 'r--', label='Textbook-Perceptron: y = 3.31x + 0.026')\n",
    "    plt.plot(x_values, y_textbook_2, 'y-.', label='Textbook-Discriminant: y = 2.5x + 106.9')\n",
    "    \n",
    "    plt.xlabel('Ring diameter—freshwater')\n",
    "    plt.ylabel('Ring diameter—saltwater')\n",
    "    plt.title('Perceptron Classification Boundaries Comparison')\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1.0))\n",
    "    plt.show()\n",
    "\n",
    "# Plot the final classification boundary with textbook lines\n",
    "plot_decision_boundary(X, y, w, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fb5fb2111edfc9",
   "metadata": {},
   "source": [
    "### Method 2\n",
    "\n",
    "- The features were normalised so that each feature had a mean of 0 and a standard deviation of 1.\n",
    "- Weights and biases are initialised close to the target boundaries, based on a priori knowledge and target classification boundaries (Textbook - The perceptron classification boundary: x2 = 3.31x1 + 0.026).\n",
    "- The learning rate is slightly lower (0.05) and more suitable for fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20a53ccd5e4a31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T06:37:39.929372Z",
     "start_time": "2024-08-21T06:37:39.730760Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fractions import Fraction\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('Fish_data.csv')\n",
    "X = df.iloc[:, 0:2].values  # Input features\n",
    "y = df.iloc[:, 2].values    # Labels (0: Canadian, 1: Alaskan)\n",
    "\n",
    "# Normalize features\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "X_scaled = (X - X_mean) / X_std\n",
    "\n",
    "# Initialize weights and bias closer to the target boundary\n",
    "initial_slope = 3.31\n",
    "intercept = 0.026\n",
    "w = np.array([initial_slope, -1])  # Initial weights approximating the target slope\n",
    "b = intercept\n",
    "\n",
    "learning_rate = 0.05  # Optimized learning rate\n",
    "epochs = 200\n",
    "\n",
    "# Perceptron learning rule\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X_scaled)):\n",
    "        # Compute the linear combination and apply the step function\n",
    "        z = np.dot(X_scaled[i], w) + b\n",
    "        pred = 1 if z >= 0 else 0\n",
    "        # Update weights and bias if there is a misclassification\n",
    "        if y[i] != pred:\n",
    "            error = y[i] - pred\n",
    "            w += learning_rate * error * X_scaled[i]\n",
    "            b += learning_rate * error\n",
    "\n",
    "# Define function to plot decision boundary and textbook equations\n",
    "def plot_decision_boundary(X, y, w, b, X_mean, X_std):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='blue', marker='o', label='Canadian')\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='orange', marker='x', label='Alaskan')\n",
    "\n",
    "    # Plot perceptron boundary\n",
    "    x_values = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), 200)\n",
    "    \n",
    "    # slope = -w[0]/w[1]\n",
    "    # intercept = -b/w[1]\n",
    "    # y_values = slope * (x_values - X_mean[0]) / X_std[0] * X_std[1] + X_mean[1] + intercept\n",
    "    # plt.plot(x_values, y_values, 'g', label=f'Chan Meng Trained: y = {slope:.2f}x + {intercept:.2f}')\n",
    "    \n",
    "    # fractions.Fraction is used to convert the results of weight and bias calculations into fraction form and limit the size of the denominator of the fraction via the limit_denominator() method to keep the output concise and readable.\n",
    "    slope = Fraction(-w[0]/w[1]).limit_denominator()\n",
    "    intercept = Fraction(-b/w[1]).limit_denominator()\n",
    "    y_values = slope * (x_values - X_mean[0]) / X_std[0] * X_std[1] + X_mean[1] + intercept\n",
    "    plt.plot(x_values, y_values, 'g', label=f'Chan Meng Trained: y = {slope}x + {intercept}')\n",
    "    \n",
    "    # Textbook equations\n",
    "    y_textbook_1 = 0.026 + 3.31 * x_values\n",
    "    y_textbook_2 = 106.9 + 2.5 * x_values\n",
    "    plt.plot(x_values, y_textbook_1, 'r--', label='Textbook-Perceptron: y = 3.31x + 0.026')\n",
    "    plt.plot(x_values, y_textbook_2, 'y-.', label='Textbook-Discriminant: y = 2.5x + 106.9')\n",
    "    \n",
    "    plt.xlabel('Ring diameter—freshwater')\n",
    "    plt.ylabel('Ring diameter—saltwater')\n",
    "    plt.title('Perceptron Classification Boundaries Comparison')\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1.0))\n",
    "    plt.show()\n",
    "\n",
    "# Plot the final classification boundary with textbook lines\n",
    "plot_decision_boundary(X, y, w, b, X_mean, X_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e00be912e67f504",
   "metadata": {},
   "source": [
    "In summary, the following are the linear equations obtained by these two methods and their analyses:\n",
    "\n",
    "### Linear equations obtained using method 1\n",
    "- **Fractional Form**：$$ y = \\frac{4922}{1399}x $$\n",
    "- **Decimal Format**：$$ y = 3.5225x $$\n",
    "\n",
    "### Linear equations obtained using method 2\n",
    "- **Fractional Form**：$$ y = \\frac{1906053}{667529}x - \\frac{314082}{742297} $$\n",
    "- **Decimal Format**：$$ y = 2.8537x - 0.4231 $$\n",
    "\n",
    "### Comparison of classification boundary lines\n",
    "Compare this with the **Textbook-Perceptron** equation (the perceptron classification boundary (2.15a)) $$ y = 3.31x + 0.026 $$. This equation misclassifies 1 Canadian point to the Alaskan side of the equation, and given that there are 47 data in total for Alaskan and 45 data in total for Canadian, the correct classification rate is:\n",
    "$$ \\text{Accuracy} = \\frac{45 - 1 + 47}{45 + 47} \\times 100\\% = 98.91\\% $$\n",
    "\n",
    "### Percentage of correct classifications for method 1\n",
    "The straight line of Method 1 misclassified three Canadian points to the Alaskan side and one other Canadian point on the classification boundary line, which was calculated to be correctly classified:\n",
    "$$ \\text{Accuracy} = \\frac{45 - 3 - 0.5 + 47}{45 + 47} \\times 100\\% \\approx 95.45\\% $$\n",
    "\n",
    "### Percentage of correct classifications for method 2\n",
    "The straight line of Method 2 misclassified only 1 Canadian point to the Alaskan side, with a correct classification rate:\n",
    "$$ \\text{Accuracy} = \\frac{45 - 1 + 47}{45 + 47} \\times 100\\% = 98.91\\% $$\n",
    "\n",
    "### Conclusion \n",
    "\n",
    "Method 2 generated more accurate classification boundary lines. The reasons for this improved accuracy include:\n",
    "\n",
    "1. **Feature standardisation**: by standardising the input features, Method 2 makes it easier for the model to learn and adapt to data at different scales, thus improving its generalisation ability. \n",
    "2. **Initial weights close to ideal boundaries**: in Method 2, the selection of initial weights and biases is more scientific, based on knowledge of the data distribution and the objective function, thus enabling faster and more accurate adjustment to the optimal classification boundaries.\n",
    "\n",
    "Method 2 not only takes into account the standardised processing of data, but also makes the final classification boundary more accurate by reasonably initialising the weights and biases. Therefore, it provides a more accurate and practical model for the classification task.\n",
    "\n",
    "### Reference:\n",
    "\n",
    "Samarasinghe, S. (2006). *Neural networks for applied sciences and engineering: From fundamentals to complex pattern recognition*. Auerbach Publishers, Incorporated. (pp. 39). Formulas 2.15a and 2.15b in Chapter 2, \"Fundamentals of Neural Networks and Models for Linear Data Analysis.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
